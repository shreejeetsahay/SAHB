{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wAVGXKojMzLo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "d0bb6fb5-cb50-4dbc-daa5-4b0af4d23f1d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "mount failed",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-9a3aa737d664>\u001b[0m in \u001b[0;36m<cell line: 46>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'pip install influxdb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;34m'https://research.google.com/colaboratory/faq.html#drive-timeout'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         )\n\u001b[0;32m--> 277\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'mount failed'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mextra_reason\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mcase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m       \u001b[0;31m# Terminate the DriveFS binary before killing bash.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: mount failed"
          ]
        }
      ],
      "source": [
        "\n",
        "import calendar\n",
        "from copy import deepcopy\n",
        "import csv\n",
        "import datetime\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.cluster import DBSCAN\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.tree import export_graphviz\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from pandas.tseries.holiday import USFederalHolidayCalendar as calendar\n",
        "\n",
        "from pylab import rcParams\n",
        "\n",
        "from scipy.stats import pearsonr\n",
        "from scipy.stats import spearmanr\n",
        "from scipy import stats\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "import pytz\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')\n",
        "\n",
        "!pip install influxdb\n",
        "import influxdb\n",
        "\n",
        "HOST = 'influx.linklab.virginia.edu'\n",
        "PORT = 443\n",
        "USERNAME = 'cps1f23'\n",
        "PASSWORD = 'phah7goohohng5ooL9mae1quohpei1Ahsh1uGing'\n",
        "DATABASE = 'gateway-generic'\n",
        "\n",
        "client = influxdb.InfluxDBClient(HOST, PORT, USERNAME, PASSWORD, DATABASE, ssl=True, verify_ssl=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run CO2 clustering and Power classification without data smoothing."
      ],
      "metadata": {
        "id": "iXvryl9P7UHp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "full_241_df = pd.read_csv(\"/content/drive/MyDrive/2024_Fall/Smart and Healthy Buildings/SAHB Energy Occupancy Group/241_data_1.csv\")\n",
        "full_243_df = pd.read_csv(\"/content/drive/MyDrive/2024_Fall/Smart and Healthy Buildings/SAHB Energy Occupancy Group/243_data_1.csv\")\n",
        "full_269_df = pd.read_csv(\"/content/drive/MyDrive/2024_Fall/Smart and Healthy Buildings/SAHB Energy Occupancy Group/269_data_1.csv\")\n",
        "full_245_df = pd.read_csv(\"/content/drive/MyDrive/2024_Fall/Smart and Healthy Buildings/SAHB Energy Occupancy Group/245_data_1.csv\")\n",
        "\n",
        "dataframes = {\n",
        "    \"Room 241\": full_241_df,\n",
        "    \"Room 243\": full_243_df,\n",
        "    \"Room 269\": full_269_df,\n",
        "    \"Room 245\": full_245_df,\n",
        "}"
      ],
      "metadata": {
        "id": "pxwscUKYM-Wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def df_list(dataframes):\n",
        "    updated_dfs = []\n",
        "    for df_name, df in dataframes.items():\n",
        "      df[\"time\"] = pd.to_datetime(df[\"time\"])\n",
        "      df['hour_of_day'] = df['time'].dt.hour\n",
        "      dataframes[df_name] = df"
      ],
      "metadata": {
        "id": "C6JF7RHJuDvJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_histogram(df_name, data, data_type, data_unit):\n",
        "    plt.hist(data, bins='auto')\n",
        "\n",
        "    # Add df_name to the title\n",
        "    plt.title(f\"Histogram of {data_type} data - {df_name}\")\n",
        "    plt.xlabel(f\"{data_type} ({data_unit})\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.show()\n",
        "\n",
        "def graph_mean_std(df_name, data, data_type, data_unit):\n",
        "    mean = np.mean(data).item()\n",
        "    std_dev = np.std(data)\n",
        "\n",
        "    # Create figure and axis\n",
        "    fig, ax = plt.subplots()\n",
        "\n",
        "    # Plot histogram\n",
        "    ax.hist(data, bins=30, edgecolor='black', alpha=0.7, label='Data')\n",
        "\n",
        "    # Add vertical lines and text for the three-sigma rule\n",
        "    for i in range(1, 4):\n",
        "        ax.axvline(mean + i * std_dev, color='red', linestyle='--', label=f'+{i}σ' if i == 1 else '')\n",
        "        ax.axvline(mean - i * std_dev, color='blue', linestyle='--', label=f'-{i}σ' if i == 1 else '')\n",
        "        ax.text(mean + i * std_dev, ax.get_ylim()[1] * 0.8, f'+{i}σ', color='red', ha='center')\n",
        "        ax.text(mean - i * std_dev, ax.get_ylim()[1] * 0.8, f'-{i}σ', color='blue', ha='center')\n",
        "\n",
        "    # Add vertical line and text for the mean\n",
        "    ax.axvline(mean, color='green', linestyle='-', label='Mean')\n",
        "    ax.text(mean, ax.get_ylim()[1] * 0.9, 'Mean', color='green', ha='center')\n",
        "\n",
        "    ax.legend()\n",
        "\n",
        "    # Add df_name to the title\n",
        "    plt.title(f\"{data_type} data - {df_name}\")\n",
        "    plt.xlabel(f\"{data_type} ({data_unit})\")\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "BytYBAeA3qZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for df_name, df in dataframes.items():\n",
        "  power_data = df['power']\n",
        "  create_histogram(df_name, power_data, \"Power\", \"W\")\n",
        "  graph_mean_std(df_name, power_data, \"Power\", \"W\")\n",
        "\n",
        "  co2_data = df['co2']\n",
        "  create_histogram(df_name, co2_data, \"CO2\", \"ppm\")\n",
        "  graph_mean_std(df_name, co2_data, \"CO2\", \"ppm\")"
      ],
      "metadata": {
        "id": "S1h_xHhkY617"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# K-means with 2 clusters\n",
        "def apply_kmeans(data, num_clusters=2):\n",
        "    kmeans = KMeans(n_clusters=num_clusters, random_state=0)\n",
        "    kmeans.fit(data)\n",
        "    labels = kmeans.labels_\n",
        "    return kmeans, labels\n",
        "\n",
        "def graph_k_means(df_name, data, data_type, kmeans, labels):\n",
        "  centroids = kmeans.cluster_centers_\n",
        "  occupied_label = np.argmax(centroids)  # Index of the higher centroid\n",
        "  binary_labels = np.where(labels == occupied_label, 1, 0)\n",
        "\n",
        "  time = np.arange(len(data))\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.scatter(time, data, c=labels, cmap='viridis', marker='o', alpha=0.6)\n",
        "  plt.xlabel('Time')\n",
        "  plt.ylabel(f'{data_type} Levels')\n",
        "  plt.title(f'K-means Clustering of {data_type} Levels Over Time for {df_name}')\n",
        "  plt.colorbar(label='Cluster')\n",
        "  plt.show()\n",
        "\n",
        "  plt.figure(figsize=(10, 6))\n",
        "  plt.hist(data[labels == 0], bins=30, alpha=0.6, label='Cluster 0')\n",
        "  plt.hist(data[labels == 1], bins=30, alpha=0.6, label='Cluster 1')\n",
        "  plt.xlabel(f'{data_type} Levels')\n",
        "  plt.ylabel('Frequency')\n",
        "  plt.title(f'Histogram of {data_type} Levels by Cluster for {df_name}')\n",
        "  plt.legend()\n",
        "  plt.show()\n",
        "\n",
        "def run_random_forest(X_train, X_test, y_train, y_test, binary_labels):\n",
        "  # Initialize the random forest classifier with previously found best hyper params\n",
        "  rf_classifier = RandomForestClassifier(\n",
        "      max_depth=None,\n",
        "      min_samples_split=2,\n",
        "      n_estimators=200,\n",
        "      random_state=42)\n",
        "\n",
        "  # Train the model\n",
        "  rf_classifier.fit(X_train, y_train)\n",
        "\n",
        "  # Make predictions on the test set\n",
        "  y_pred = rf_classifier.predict(X_test)\n",
        "\n",
        "  # Calculate accuracy\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  print(f\"Accuracy: {accuracy:.2f}\")\n",
        "\n",
        "  # Print classification report for more insights\n",
        "  print(\"Classification Report:\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  # Confusion matrix\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "  return rf_classifier"
      ],
      "metadata": {
        "id": "eHfIBAVc0JIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for df_name, df in dataframes.items():\n",
        "    co2_data = df['co2']\n",
        "    power_data = df['power']\n",
        "\n",
        "    # using min since it gave us the best results (max also works)\n",
        "    co2_min = co2_data.rolling(window=10).max()\n",
        "    power_min = power_data.rolling(window=10).max()\n",
        "\n",
        "    co2_min = co2_min[9:].to_numpy().reshape(-1, 1)  # Reshape for scaling and clustering\n",
        "    power_min = power_min[9:].to_numpy().reshape(-1, 1)  # Reshape for scaling and clustering\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    co2_min_scaled = scaler.fit_transform(co2_min)\n",
        "    power_min_scaled = scaler.fit_transform(power_min)\n",
        "\n",
        "    kmeans, binary_labels = apply_kmeans(co2_min_scaled, num_clusters=2)\n",
        "    graph_k_means(df_name, co2_min_scaled, \"CO2\", kmeans, binary_labels)\n",
        "\n",
        "    X = power_min_scaled\n",
        "    y = binary_labels\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    rf_classifier = run_random_forest(X_train, X_test, y_train, y_test, binary_labels)"
      ],
      "metadata": {
        "id": "3f-FSdMt5sQN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}